{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 詞表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/worddata.txt\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()\n",
    "lines[0] = lines[0].replace(\"\\ufeff\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar<=u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "word_dic={}\n",
    "for l in lines:\n",
    "    wordlist = l.split(\"&\")\n",
    "    if len(wordlist)!=4:\n",
    "        continue\n",
    "    for w in list(wordlist[0].replace(\" \",\"\")):\n",
    "        if is_chinese(w)== False:\n",
    "            continue\n",
    "        if w in word_dic.keys():\n",
    "            word_dic[w]+= int(wordlist[2])\n",
    "        else:\n",
    "            word_dic[w] = int(wordlist[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/firstword.txt\", \"wt\",encoding=\"utf-8\") as dic:\n",
    "    sorted_word_dict = sorted(word_dic.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    for w in sorted_word_dict:\n",
    "        dic.write(w[0]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/firstword.txt\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    txt = fin.read()\n",
    "firstword = txt.split(\"\\n\")\n",
    "del firstword[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 錯誤實例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "with open(\"data/wrong1.txt\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    txt = fin.read()\n",
    "soup = BeautifulSoup(txt, \"lxml\")\n",
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar<=u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WrongWord = {}\n",
    "for w in firstword:\n",
    "    WrongWord[w] = {}\n",
    "\n",
    "for essay in soup.select(\"essay\"):\n",
    "    mistake = essay.select(\"mistake\")\n",
    "    for m in mistake:\n",
    "        # 錯字行字數不等於正確行，這是多或少某字而非錯字，所以跳過\n",
    "        if len(m.select(\"wrong\")[0].text)!= len(m.select(\"correction\")[0].text): \n",
    "            continue\n",
    "        location = int(m[\"location\"])-1\n",
    "        wrong = essay.find(id = m[\"id\"]).text[location]\n",
    "        wrong_index = m.select(\"wrong\")[0].text.index(wrong)\n",
    "        correct = m.select(\"correction\")[0].text[wrong_index]\n",
    "        if wrong == correct:\n",
    "            continue\n",
    "        if is_chinese(wrong) == False or is_chinese(correct) == False: # 非中文字跳過\n",
    "            continue\n",
    "      \n",
    "        if correct not in WrongWord.keys(): #不在4743字的不收\n",
    "            continue\n",
    "        else:\n",
    "            if wrong not in WrongWord[correct].keys(): \n",
    "                WrongWord[correct][wrong] = []\n",
    "                WrongWord[correct][wrong].append(1)\n",
    "                try:\n",
    "                    WrongWord[correct][wrong].append((firstword.index(wrong)+1)*-1) # 加入wrong在firstword的index，但因為排序問題要先加1(為了0)乘-1\n",
    "                except:\n",
    "                    WrongWord[correct][wrong].append(-4745)#wrong 不在firstword裡的wrong統一加在最後\n",
    "            else:\n",
    "                WrongWord[correct][wrong][0]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wrong2.txt\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    txt = fin.read()\n",
    "soup = BeautifulSoup(txt, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in soup.select(\"doc\"):\n",
    "    mistake = doc.select(\"mistake\")\n",
    "    for m in mistake:\n",
    "        # 錯字行字數不等於正確行，這是多或少某字而非錯字，所以跳過\n",
    "        if len(m.select(\"wrong\")[0].text)!= len(m.select(\"correct\")[0].text): \n",
    "            continue\n",
    "        location = int(m[\"wrong_position\"])-1\n",
    "        wrong = doc.select(\"p\")[0].text[location]\n",
    "        wrong_index = m.select(\"wrong\")[0].text.index(wrong)\n",
    "        correct = m.select(\"correct\")[0].text[wrong_index]\n",
    "        if wrong == correct:\n",
    "            continue\n",
    "            print(wrong)\n",
    "        if is_chinese(wrong) == False or is_chinese(correct) == False: # 非中文字跳過\n",
    "            continue\n",
    "      \n",
    "        if correct not in WrongWord.keys(): #不在4743字的不收\n",
    "            continue\n",
    "        else:\n",
    "            if wrong not in WrongWord[correct].keys(): \n",
    "                WrongWord[correct][wrong] = []\n",
    "                WrongWord[correct][wrong].append(1)\n",
    "                try:\n",
    "                    WrongWord[correct][wrong].append((firstword.index(wrong)+1)*-1) # 加入wrong在firstword的index，但因為排序問題要先加1(為了0)乘-1\n",
    "                except:\n",
    "                    WrongWord[correct][wrong].append(-4745)#wrong 不在firstword裡的wrong統一加在最後\n",
    "            else:\n",
    "                WrongWord[correct][wrong][0]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "standardizeWW = OrderedDict()\n",
    "n= 0 \n",
    "for word in firstword:\n",
    "    word_dict =WrongWord[word]\n",
    "    if word_dict == {}:\n",
    "        standardizeWW[word] = {}\n",
    "        continue\n",
    "    sorted_word_dict = sorted(word_dict.items(), key=lambda kv: (kv[1][0],kv[1][1]), reverse=True)\n",
    "    standardizeWW [word] = OrderedDict()\n",
    "    for s in sorted_word_dict:\n",
    "        if s[1][0]>=10:\n",
    "            n+=1\n",
    "            standardizeWW [word][s[0]] = s[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全字庫\n",
    "https://www.cns11643.gov.tw/\n",
    "* 獲得字音、部件、筆畫資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/CNS.txt\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    txt = fin.readlines()\n",
    "CNS={}\n",
    "for t in txt:\n",
    "    split = t[:-1].split(\",\")\n",
    "    CNS[split[0]] = split[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/CNS/one_phonetic.xml\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    txt = fin.read()\n",
    "soup = BeautifulSoup(txt, \"lxml\")\n",
    "#統一拼音\n",
    "one_phonetic = {}\n",
    "for w in firstword:\n",
    "    one_phonetic[w] = {}\n",
    "for s in soup.select(\"word\"):\n",
    "    phone = s.find(\"phone\").text.split(\"\\u3000\")\n",
    "    word = list(s[\"item\"])\n",
    "    \n",
    "    for w,p in zip(word,phone):\n",
    "        if w not in one_phonetic.keys():\n",
    "            continue\n",
    "        else:\n",
    "            if p not in one_phonetic[w].keys(): \n",
    "                one_phonetic[w][p] = 1\n",
    "            else:\n",
    "                one_phonetic[w][p] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initials = [\"ㄅ\",\"ㄆ\",\"ㄇ\",\"ㄈ\",\"ㄉ\",\"ㄊ\",\"ㄋ\",\"ㄌ\",\"ㄍ\",\"ㄎ\",\"ㄏ\",\"ㄐ\",\"ㄑ\",\"ㄒ\",\"ㄓ\",\"ㄔ\",\"ㄕ\",\"ㄖ\",\"ㄗ\",\"ㄘ\",\"ㄙ\"]\n",
    "finals = [\"ㄚ\",\"ㄛ\",\"ㄜ\",\"ㄝ\",\"ㄟ\",\"ㄞ\",\"ㄠ\",\"ㄡ\",\"ㄢ\",\"ㄤ\",\"ㄣ\",\"ㄥ\",\"ㄦ\"]\n",
    "jiein = [\"ㄧ\",\"ㄨ\",\"ㄩ\"]\n",
    "tone = [\"˙\",\"-\",\"ˊ\",\"ˇ\",\"ˋ\"]\n",
    "with open(\"data/CNS/CNS_phonetic.txt\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    txt = fin.readlines()\n",
    "#拼音\n",
    "phonetic={}\n",
    "for t in txt:\n",
    "    p = t[:-1].replace(\"\\t\",\"-\",1).split(\"\\t\")\n",
    "    yin = [\"#\",\"#\",\"#\",\"1\"]\n",
    "    for w in p[1]:\n",
    "        if w in initials:\n",
    "            yin[0] = w\n",
    "        elif w in jiein:\n",
    "            yin[1] = w\n",
    "        elif w in finals:\n",
    "            yin[2] = w    \n",
    "        elif w in tone:\n",
    "            yin[3] = str(tone.index(w))\n",
    "    p[1] = \"\".join(yin)\n",
    "    if p[0] in phonetic.keys():\n",
    "        phonetic[p[0]].append(p[1])\n",
    "    else:\n",
    "        phonetic[p[0]] = []\n",
    "        phonetic[p[0]].append(p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/CNS/CNS_stroke.txt\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    txt = fin.readlines()\n",
    "#筆畫\n",
    "stroke={}\n",
    "for t in txt:\n",
    "    c = t[:-1].replace(\"\\t\",\"-\",1).split(\"\\t\")\n",
    "    stroke[c[0]] = int(c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/CNS/CNS_component.txt\", \"rt\",encoding=\"utf-8\") as fin:\n",
    "    txt = fin.readlines()\n",
    "#部件\n",
    "component={}\n",
    "for t in txt:\n",
    "    c = t[:-1].replace(\"\\t\",\"-\",1).split(\"\\t\")\n",
    "    component[c[0]] = []\n",
    "    if \";\" in c[1]:\n",
    "        another = c[1].split(\";\")\n",
    "        for a in another:\n",
    "            component[c[0]].append(a.split(\",\"))\n",
    "    else:\n",
    "        component[c[0]].append(c[1].split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNS_word = {}\n",
    "for f in firstword:\n",
    "    CNS_word[f] = []\n",
    "    #component\n",
    "    CNS_word[f].append(component[CNS[f]])\n",
    "    #phonetic\n",
    "    if len(phonetic[CNS[f]])>1:\n",
    "        try:\n",
    "            p = max(one_phonetic[f], key=one_phonetic[f].get)\n",
    "            yin = [\"#\",\"#\",\"#\",\"1\"]\n",
    "            for w in p:\n",
    "                if w in initials:\n",
    "                    yin[0] = w\n",
    "                elif w in jiein:\n",
    "                    yin[1] = w\n",
    "                elif w in finals:\n",
    "                    yin[2] = w    \n",
    "                elif w in tone:\n",
    "                    yin[3] = str(tone.index(w))\n",
    "            p = \"\".join(yin)\n",
    "            CNS_word[f].append(p)\n",
    "            \n",
    "        except:\n",
    "            CNS_word[f].append(phonetic[CNS[f]][0])\n",
    "    else:\n",
    "        CNS_word[f].append(phonetic[CNS[f]][0])\n",
    "    #stroke\n",
    "    CNS_word[f].append(stroke[CNS[f]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 部件\n",
    "* 順序：jaccard相似度>筆畫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(a, b):\n",
    "    unions = len(set(a).union(set(b)))\n",
    "    intersections = len(set(a).intersection(set(b)))\n",
    "    return 1. * intersections / unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = {} #詞表字與其他詞表字的jaccard相似度\n",
    "for w in CNS_word:  \n",
    "    jaccard[w] = {}\n",
    "    for i in CNS_word:\n",
    "        jaccard[w][i] = [0,abs(CNS_word[i][2]-CNS_word[w][2])*(-1)] #相似度(預留),筆畫差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in CNS_word:  #w:詞表字  i:對應的四千七 n:多個部件\n",
    "    if len(CNS_word[w][0]) ==1: #部件組成方式僅有一種\n",
    "        jac = CNS_word[w][0][0]\n",
    "        for i in CNS_word:\n",
    "            jaccard[w][i][0]=jaccard_sim(jac,CNS_word[i][0][0])\n",
    "            \n",
    "    else: #部件組成方式超過一種 取相似度最大的一種\n",
    "        for jac_word in CNS_word:\n",
    "            for fs1 in CNS_word[w][0]:\n",
    "                for fs2 in CNS_word[jac_word][0]: \n",
    "                    jac = jaccard_sim(fs1,fs2)\n",
    "                    if  jac > jaccard[w][jac_word][0]:\n",
    "                        jaccard[w][jac_word][0] = jac\n",
    "                    jaccard[jac_word][w][0] = jaccard[w][jac_word][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#刪去相似度小於0.5及本身字\n",
    "for jac in jaccard:\n",
    "    for j in firstword: \n",
    "        if jaccard[jac][j][0] < 0.5 or jac==j:\n",
    "            del jaccard[jac][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "#排序\n",
    "standardizeCP = OrderedDict()\n",
    "for word in firstword:\n",
    "    word_dict = jaccard[word]\n",
    "    if word_dict == {}:\n",
    "        standardizeCP[word] = {}\n",
    "        continue\n",
    "    sorted_word_dict = sorted(word_dict.items(), key=lambda kv: (kv[1][0],kv[1][1]), reverse=True)\n",
    "    standardize_word = OrderedDict()\n",
    "    for s in sorted_word_dict:\n",
    "        standardize_word[s[0]] = s[1][0] \n",
    "    standardizeCP[word] = standardize_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拼音\n",
    "* 聲母+介音+韻母+聲調+常見程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def distance(a,b):\n",
    "    d = 0\n",
    "    for x,y in zip(a,b):\n",
    "        d += (x-y)**2\n",
    "    return math.sqrt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "init ={\"ㄅ\":(1,1),\"ㄆ\":(1,2),\"ㄇ\":(1,5),\"ㄈ\":(2,7),\"ㄉ\":(3,1),\"ㄊ\":(3,2),\"ㄋ\":(3,5),\"ㄌ\":(3,6),\n",
    " \"ㄍ\":(4,1),\"ㄎ\":(4,2),\"ㄏ\":(4,7),\"ㄐ\":(5,3),\"ㄑ\":(5,4),\"ㄒ\":(5,7),\"ㄓ\":(6,3),\"ㄔ\":(6,4),\n",
    " \"ㄕ\":(6,7),\"ㄖ\":(6,8),\"ㄗ\":(7,3),\"ㄘ\":(7,4),\"ㄙ\":(7,7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "initials_distance={} #聲母距離\n",
    "for p in init:\n",
    "    initials_distance[p] = {} \n",
    "    for i in init:\n",
    "        initials_distance[p][i] = distance(init[p],init[i])\n",
    "    initials_distance[p][\"#\"] = 9\n",
    "initials_distance[\"#\"] = {}#零聲母\n",
    "for p in init:\n",
    "    initials_distance[\"#\"][p] = 9\n",
    "    initials_distance[\"#\"][\"#\"] = 1\n",
    "#標準化\n",
    "for w in initials_distance:\n",
    "    Max = max(initials_distance[w].values())\n",
    "    Min = min(initials_distance[w].values())\n",
    "    for i in initials_distance[w]:\n",
    "        initials_distance[w][i] = 1-((initials_distance[w][i]-Min)/(Max-Min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel={\"i\":(1,1,0),\"y\":(1,1,1),\"u\":(3,1,1),\"e\":(1,3,0),\"ɤ\":(3,3,0),\"o\":(3,3,1),\"ə\":(2,4,0),\"a\":(1,7,0),\"ɑ\":(3,7,0)}\n",
    "fin = {'ㄚ':\"a\", 'ㄛ':\"o\", 'ㄜ':\"ɤ\", 'ㄝ':\"e\", 'ㄟ':\"ei\", 'ㄞ':\"ai\", 'ㄠ':\"ɑu\", 'ㄡ':\"ou\", 'ㄢ':\"an\", 'ㄤ':\"ɑŋ\", 'ㄣ':\"ən\", 'ㄥ':\"əŋ\", 'ㄦ':\"ɚ\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_finals_distance(A,B):\n",
    "    dist=0\n",
    "    n=0\n",
    "    for a in fin[A]:\n",
    "        for b in fin[B]:\n",
    "            n+=1\n",
    "            if A == B:\n",
    "                dist = 0\n",
    "                continue            \n",
    "            if a in \"ɚ\" or b==\"ɚ\":\n",
    "                dist += 7\n",
    "            elif a in (\"n\",\"ŋ\") and b not in (\"n\",\"ŋ\"):\n",
    "                dist += 7\n",
    "            elif a not in (\"n\",\"ŋ\") and b in (\"n\",\"ŋ\"):\n",
    "                dist += 7\n",
    "            elif a in (\"n\",\"ŋ\") and b in (\"n\",\"ŋ\"):\n",
    "                if a == b:\n",
    "                    dist +=0\n",
    "                else:\n",
    "                    dist += 1\n",
    "            else:\n",
    "                dist += distance(vowel[a],vowel[b])\n",
    "    return dist/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "finals_distance={}#韻母發音距離\n",
    "for f in fin:\n",
    "    finals_distance[f] = {} \n",
    "    for i in fin:\n",
    "        finals_distance[f][i] =count_finals_distance(f,i)\n",
    "    finals_distance[f][\"#\"] = 7\n",
    "finals_distance[\"#\"] = {}\n",
    "for f in fin:\n",
    "    finals_distance[\"#\"][f] = 7\n",
    "    finals_distance[\"#\"][\"#\"] = 1\n",
    "#標準化\n",
    "for w in finals_distance:\n",
    "    Max = max(finals_distance[w].values())\n",
    "    Min = min(finals_distance[w].values())\n",
    "    for i in finals_distance[w]:\n",
    "        finals_distance[w][i] = 1-((finals_distance[w][i]-Min)/(Max-Min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phonetic = {}\n",
    "L = len(firstword)\n",
    "for w in firstword:\n",
    "    firstword_phonetic = CNS_word[w][1]\n",
    "    Phonetic[w] = {}\n",
    "    for i in firstword:\n",
    "        if w == i:\n",
    "            continue\n",
    "        word_phonetic = CNS_word[i][1]\n",
    "        I = initials_distance[firstword_phonetic[0]][word_phonetic[0]] #聲母\n",
    "        if firstword_phonetic[1] == word_phonetic[1]: #介音\n",
    "            J = 1\n",
    "        else:\n",
    "            J = 0\n",
    "        F = finals_distance[firstword_phonetic[2]][word_phonetic[2]] # 韻母\n",
    "        T = 1 - abs(int(firstword_phonetic[3])- int(word_phonetic[3]))/4 #聲調\n",
    "        O = (1-firstword.index(i)/L)/2\n",
    "        Phonetic[w][i] = (I+J+F+T+O)/4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "standardizePH = OrderedDict()\n",
    "for f in firstword:\n",
    "    standardizePH[f] = OrderedDict()\n",
    "    sorted_dict = sorted(Phonetic[f].items(), key=lambda kv: kv[1], reverse=True)\n",
    "    for s in sorted_dict[:10]:\n",
    "        standardizePH[f][s[0]] = s[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = {}\n",
    "for f in firstword:\n",
    "    Final[f] = {}\n",
    "    for w in standardizeWW[f]:\n",
    "        Final[f][w] = standardizeWW[f][w]\n",
    "    for c in standardizeCP[f]:\n",
    "        if c in Final[f].keys():\n",
    "            Final[f][c] += standardizeCP[f][c]\n",
    "        else:\n",
    "            Final[f][c] = standardizeCP[f][c]\n",
    "    for p in standardizePH[f]:\n",
    "        if p in Final[f].keys():\n",
    "            Final[f][p] += standardizePH[f][p]\n",
    "        else:\n",
    "            Final[f][p] = standardizePH[f][p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ConfusionSet.txt\", \"wt\",encoding=\"utf-8\") as fin:\n",
    "    for f in Final:\n",
    "        word_dict =Final[f]\n",
    "        sorted_word_dict = sorted(word_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        sorted_word=\"\"\n",
    "        for s in sorted_word_dict[:10]:\n",
    "            sorted_word +=s[0]\n",
    "        fin.write(f+\",\"+sorted_word+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
